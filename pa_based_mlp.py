# -*- coding: utf-8 -*-
"""PA_based_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ps8JPX8OwQqo0NTsEXP46n5s0DeN7GMV
"""

class Pooling_attention(nn.Module):
  def __init__(self,channel,kernel_size=5):
    super(Pooling_attention, self).__init__()
    self.adapavg = nn.AdaptiveAvgPool2d(1)
    self.adapmax = nn.AdaptiveMaxPool2d(1)
    self.conv = nn.Conv1d(1,1,kernel_size=kernel_size,padding=kernel_size//2)
    self.norm = nn.Sequential(
        # nn.BatchNorm2d(channel),
        nn.Sigmoid()
    )
    self.fc = nn.Sequential(
            nn.Linear(channel, channel // 8, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // 8, channel, bias=False),
            nn.Sigmoid(),
        )
    self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
  def forward(self,x):
    b,c,h,w = x.shape
    out = x
    x1 = nn.AdaptiveAvgPool2d((h//2,w//2))(x)
    x2 = nn.AdaptiveMaxPool2d((h//2,w//2))(x)
    out = x1+x2
    out1 =  self.adapavg(x)
    out1 = out1.view(out1.size(0), -1)
    out1 = self.fc(out1).view(b, c, 1, 1)
    out1 = out*out1
    out2 = self.adapmax(x)
    out2 = out2.view(out2.size(0), -1)
    out2 = self.fc(out2).view(b, c, 1, 1)
    out2 = out*out2

    output = out1+out2
    return output